{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 선형 회귀 요약\n",
    "\n",
    "| 특징 | 종류 및 활용범위 | 규제 | 평가 지표 |\n",
    "| --- | --- | --- | --- |\n",
    "| ▶ 실제값과 예측값 차이를 최소화하는 회귀선을 구해 예측\n",
    "\n",
    "▶ 시계열 y를 예측할 때 이것이 다른 시계열 x와 선형 관계가 있다고 가정 | ▶ 단순 선형 회귀\n",
    "    → 단항식(회귀가 직선의 일차 방정식 형태)\n",
    "    → 잔차의 합이 최소가 되는 모델\n",
    "    → 비용의 최소를 최적화하는 모델\n",
    "    → 회귀계수 ( 기울기 / y절편)\n",
    "ex) 전체 광고 비용(x)라는 예측 변수로 월별 판매량 (y)\n",
    "\n",
    "▶ 다항 회귀\n",
    "    → 다항식(회귀가 직선이 아님)\n",
    "   → 2개 이상의 독립변수가 있을 경우\n",
    "   → 과적합/과소적합 발생 가능\n",
    "   → sklearn은 비선형 함수를 적용해. 다항 회귀 적용\n",
    "       Polynomial(다항식 + 차수) | ▶ Ridge\n",
    "    → 선형 회귀에 L2 규제를 추가한 회귀 모델\n",
    "\n",
    "▶ Lasso\n",
    "    → 선형 회귀에 L1 규제를 추가한 회귀 모델\n",
    "\n",
    "▶ ElasticNet\n",
    "    → L2, L1 규제를 함께 결합 | ▶ MAE : 잔차의 절대값 합의 평균\n",
    "   → 낮을 수록 좋음.\n",
    "   → sklearn API : metrics.mean_absolute__error\n",
    "   → Scoring(cross_val_score, GridSearchCV) : ‘neg_mean_absolute_error’\n",
    "\n",
    "▶ MSE : 잔차 제곱의 평균\n",
    "   → 낮을 수록 좋음.\n",
    "   → sklearn API : metrics.mean_squared__error \n",
    "   → Scoring(cross_val_score, GridSearchCV) : ‘neg_mean_squared_error’\n",
    "\n",
    "▶ RMSE : MSE의 제곱근\n",
    "   → 낮을 수록 좋음.\n",
    "   → sklearn API : metrics.mean_squared__error & (squared = False)\n",
    "   → Scoring(cross_val_score, GridSearchCV) : ‘neg_root_mean_squared_error’\n",
    "\n",
    "▶ MSLE : MSE에 Log를 취함\n",
    "   → 낮을 수록 좋음\n",
    "   → sklearn API : metrics.mean_squared__log_error\n",
    "   → Scoring(cross_val_score, GridSearchCV)  : ‘neg_mean_squared_log_error’\n",
    "\n",
    "▶ R_Squared : 예측값 분산 / 실제값 분산\n",
    "   → 1에 가까울 수록 좋음\n",
    "   → sklearn API : metrics.r2_score\n",
    "   → Scoring(cross_val_score, GridSearchCV)  : ‘r2’ |\n",
    "|  |  |  |  |\n",
    "\n",
    "# 📌 선형 회귀 비용함수 최소화\n",
    "\n",
    "## 1. 경사하강법(Gradient Descent)\n",
    "\n",
    "### 1-1) 비용 함수\n",
    "\n",
    "<aside>\n",
    "⭐\n",
    "\n",
    "![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/55341269-5ead-454a-b78f-85b00c4628af/caf22406-1273-4972-a7a2-de432e098484/image.png)\n",
    "\n",
    "$$\n",
    "R(w) = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - (w_0 + w_1 x_i) \\right)^2\n",
    "$$\n",
    "\n",
    "</aside>\n",
    "\n",
    "### 1-2) 편미분\n",
    "\n",
    "<aside>\n",
    "⭐\n",
    "\n",
    "$$\n",
    "w_1-편미분\n",
    "$$\n",
    "\n",
    "$$\n",
    "-\\frac{2}{n} \\sum_{i=1}^n \\left( y_i - (w_0 + w_1 x_i) \\right) * x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_0-편미분\n",
    "$$\n",
    "\n",
    "$$\n",
    "-\\frac{2}{n} \\sum_{i=1}^n \\left( y_i - (w_0 + w_1 x_i) \\right)\n",
    "$$\n",
    "\n",
    "</aside>\n",
    "\n",
    "### 1-3) 경사 하강 반복\n",
    "\n",
    "<aside>\n",
    "⭐\n",
    "\n",
    "$$\n",
    "w_1-업데이트\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_1=w_1−η⋅-\\frac{2}{n} \\sum_{i=1}^n \\left( y_i - (w_0 + w_1 x_i) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_0-업데이트\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_0=w_0−η⋅-\\frac{2}{n} \\sum_{i=1}^n \\left( y_i - (w_0 + w_1 x_i) \\right) * x_i\n",
    "$$\n",
    "\n",
    "**η은 학습률(learing_rate)**\n",
    "\n",
    "</aside>\n",
    "\n",
    "### 1-4) 최적화\n",
    "\n",
    "<aside>\n",
    "⭐\n",
    "\n",
    "▶ 기울기가 매우 작아져서\n",
    "\n",
    "     0에 수렴했다고 판단\n",
    "\n",
    "$$\n",
    "w_0 → 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "  w_1→0\n",
    "$$\n",
    "\n",
    "</aside>\n",
    "\n",
    "## 2. 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)\n",
    "\n",
    "랜덤하게 선택된 하나의 데이터 포인터만을 이용해 기울기 계산\n",
    "\n",
    "## 3. 배치 경사 + SGD의 하이브리드 (Mini-Batch Gradient Descent)\n",
    "\n",
    "데이터를 작은 배치로 나누어, 각 배치에 대해 기울기를 계산\n",
    "\n",
    "### cf) 참고\n",
    "\n",
    "| 방법 | 장점 | 단점 |\n",
    "| --- | --- | --- |\n",
    "| 정규 방정식 | 반복 없이 빠르게 최적화 가능 | 대규모 데이터에 비효율적 |\n",
    "| 경사 하강법 | 간단하며 널리 사용 | 수렴 속도가 느림 |\n",
    "| 뉴턴 방법 | 빠르게 수렴 가능 | 해세 행렬 계산이 비싸고 복잡 |\n",
    "| 유전 알고리즘 | 비선형 문제 해결 가능 | 계산 비용이 높음 |\n",
    "| 베이지안 최적화 | 비용 함수 평가 횟수를 줄일 수 있음 | 복잡하고 초기 설정이 필요 |\n",
    "| 시뮬레이티드 어닐링 | 전역 최적점 탐색 가능 | 느린 수렴 속도 |\n",
    "| 확률적 경사 하강법 (SGD) | 대규모 데이터셋 처리 가능 | 결과가 불안정할 수 있음 |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
